{"data":{"site":{"siteMetadata":{"title":"Blogging!","author":"Agustín Aliaga"}},"markdownRemark":{"id":"d8111216-9de8-57f9-8c37-7294b8723dbf","excerpt":"Building an eCommerce Scraper with Node.js and React.js \n Photo by  Brooke\nLark \non\n Unsplash This is a quick story about how I created a…","html":"<h1>Building an eCommerce Scraper with Node.js and React.js</h1>\n<p><img src=\"https://cdn-images-1.medium.com/max/2000/1*zq7U7ZvY70-anopuDJTyZQ.jpeg\">\n<span class=\"figcaption_hack\">Photo by <a href=\"https://unsplash.com/photos/W1B2LpQOBxA?utm_source=unsplash&#x26;utm_medium=referral&#x26;utm_content=creditCopyText\">Brooke\nLark</a>\non\n<a href=\"https://unsplash.com/search/photos/shop-online?utm_source=unsplash&#x26;utm_medium=referral&#x26;utm_content=creditCopyText\">Unsplash</a></span></p>\n<p>This is a quick story about how I created a simple but end-to-end e-commerce\nscraping web application called** “crawl-io”**. The goal behind this article is\nto provide an example of an implementation so that other projects can borrow\nideas from it. The tech stack I picked for the job was MERN. Of course, keep in\nmind that this article only reflects my personal experience and opinions. I’m\ngoing to place Gists and Screenshots across the article, but <a href=\"https://github.com/agustinaliagac/crawl-io\">check the actual\nrepo</a> in case some of it gets\noutdated. One final warning: the language of the UI text is Spanish and the\ncrawled sites are from my country (Argentina), only beacuse I didn’t get the\ntime to add others yet. The code and the concepts apply for any other e-commerce\nsite as well.</p>\n<h3>What does “crawl-io” do exactly ?</h3>\n<p>The purpose of this application is quite simple. It allows the user to type in a\nspecific product name, to crawl multiple e-commerce sites and asynchronously\nreturn useful results to the user. Some cool stuff that I didn’t include because\nof lack of time are custom filtering and caching.</p>\n<h3>UI and Architecture</h3>\n<h4>Building the front end</h4>\n<p>To make it short, the front end is really small (only two pages: one for the\nsearch box and another one for the results). The application only uses\nclient-side rendering. This could be considered as a flaw in terms of SEO, but\nit wasn’t really important in this case. React.js was really useful to create\nthe base components (with a little magic of MaterialUI). The UI itself is not\ngorgeous at all, but user friendly enough. Take a look:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/800/1*kcMaENFACILGxwWiugxOmQ.png\"></p>\n<p>I added an “infinite scrolling” feature to avoid memory leaks on the browser and\ncrashes in the application when the user receives hundreds of results.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/800/1*qYkbPmfXFccf2172CzJ4WQ.png\"></p>\n<p>Even though this app is small, I wanted to take care of state management outside\nof React itself. That’s where <a href=\"https://redux.js.org/\">Redux</a> came in handy. In\ncase you’re not familiar with it, <a href=\"https://redux.js.org/\">Redux</a> is a popular\nlibrary in the React community that takes care of application state by applying\nconcepts from Facebook’s Flux pattern. And it does the job great. I created two\nstores, separating the app in two main modules: Search and Results. I decided to\nuse the “ducks pattern” because I used it in other projects and I liked its\nending structure. Basically, this pattern consists in putting reducers, action\ncreators and thunks from a same module in a single file. I moved everything from\n‘search term’ to ‘search results items’ into Redux’s state, thus making the code\nstructure cleaner and nicer. <a href=\"https://github.com/erikras/ducks-modular-redux\">Read more about the ducks pattern here\n!</a> Asynchronous execution was\ntaken care of by Redux Thunk. Here’s the Search Results module file:</p>\n<h4>Did I need Redux at all in the first place?</h4>\n<p>Short answer: no. Longer answer: this project probably wasn’t large enough to\njustify the usage of this library. However, I wanted to create and end-to-end\nsample product, and I considered this appropriate. The beauty of Redux is that\nyou can move your state from components to actual stores, making your\narchitecture more reliable, consistent and scalable.</p>\n<h4>Moving to the back end</h4>\n<p>The back end of the project is also tiny, yet powerful. From crawl-io’s\nperspective, there are two types of sites: the ones that include a REST API for\nsearching and the ones that don’t. I called all of them <strong>providers</strong>.</p>\n<p>The ‘data-providers’ directory contains this information about the sites: What\nare their names? What type of sites are they? Which are the CSS Selectors to\nretrieve the data? Which is the endpoint that I need to call if it’s a REST API?\nAll these questions get an answer in a separate provider file. A separate\nfunction constructs the object that describes all the necessary data. This\ncollection of functions gets copied in build time to the front end side in order\nto add dynamic filtering. Here’s a sample provider file:</p>\n<p>When a search request is received on the Node.js side, we’re launching\n“concurrent” search processes for all providers. Those functions get mapped to\nPromises and we notify the results to the front end using WebSockets just as the\nPromises get resolved. This means that the user gets to see dynamic content in\nreal time. The promises can be resolved by using either a scraper function or a\nrest-client function, which are implemented with X-Ray and Axios respectively.\nThe whole process of scraping websites takes a lot of RAM memory, so I had to\ntweak the starting command to increase the heap limit for Node.js. In the middle\nof this process, we create a UUID to match the session with the search request\nitself, allowing the front end to listen to the right events. Here’s what the\nsearch function looks like:</p>\n<h3>Implementation details</h3>\n<h4>Search results ranking algorithm</h4>\n<p>Ranking results is an essential piece of any search system. It’s what will make\nthe application useful to the user or complete garbage.</p>\n<p>Say we want to search for some product like “laptop”. It’s likely that I don’t\nwant to see things like “laptop screen” or “laptop charger” at the top of the\npage. Even though they have some words in common, they are completely different\nobjects. How can we take care of that despite of not having control over the\nproviders’ search algorithms? Some other problematic factors include:</p>\n<ul>\n<li>Not having pre-stored information that could be useful to make predictions.</li>\n<li>Each result item consisting only of the following data: title, price, URL and\nthumbnail.</li>\n</ul>\n<p>My conclusion was to use a weighted ranking algorithm. By using multipliers with\ndifferent values, we can modify each item’s weight according to what we consider\nto be relevant or not. I used two multipliers:</p>\n<ol>\n<li>The closeness of the price to a central value which I called “positional\nreference”. This value is calculated as an average between the arithmetic mean\nand the median, because I considered it was representative enough.</li>\n<li>The inclusion of the search term in the title. Believe it or not, some sites\nwould return a “Fridge” when looking for a “Smartphone”.</li>\n</ol>\n<p>Finally, this is what the algorithm looks like:</p>\n<h4>ES5 or ES6 ?</h4>\n<p>If you ever read the project’s code, you’ll notice it doesn’t use the same\nstandards across the whole project. That’s because I wanted to showcase both ES6\non React.js and ES5 (with bits of ES6) on Node.js.</p>\n<p>ES6 brings good features to the table, such as classes, arrow functions,\nblock-scoped variables, template literals, enhanced object literals, object\ndestructuring, default function arguments, etc. However, I thought it would be\ninteresting to stick a little bit with ES5 on the back end to showcase the\ndifferences.</p>\n<h4>Linting and Unit Testing</h4>\n<p>ESLint was the perfect choice to do some Linting in the project. The project\nextends Airbnb’s Lint config. Moreover, the VSCode plugin for ESLint works\nextremely well. Unit testing was done by using Jest on React.js and Mocha on\nNode.js. It was interesting for me to use something like Chai, which allows us\nto write expressions in a way similar to natural language.</p>\n<h4>Data storage</h4>\n<p>Crawl-io retrieves data in real time. There’s no database to pull the\npre-indexed data from. What I did is to save each pair of results-selection in a\nMongoDB collection. In a more complete implementation, that data could be useful\nfor some sort of Machine Learning algorithm to predict the most useful way to\nrank results.</p>\n<h4>A useful CI/CD Pipeline</h4>\n<p>My CI/CD pipeline consists in these steps:</p>\n<ul>\n<li>Running all tests from React and Node</li>\n<li>Building a Docker image and <a href=\"https://hub.docker.com/r/agustinaliagac/crawlio/\">deploying it to Docker\nHub</a></li>\n<li>Deploying the project in VPS server through SSH</li>\n</ul>\n<p>All of this runs <a href=\"https://circleci.com/gh/agustinaliagac/crawl-io/\">on a public CircleCI\njob</a>. I could have picked\nTravisCI, Jenkins or others. But it had a good free plan for open source\nprojects, which supports docker builds, so I went with it.</p>\n<p>As I wrote in my previous <a href=\"https://medium.com/@agustin.aliaga/how-docker-worked-for-me-as-a-software-developer-58a049f132ff\">article about Docker from a developer’s\nperspective</a>,\nI <strong>really</strong> like Docker. I must add that it was delightful to run my scraper\nimage inside a set of different containers with Docker Compose. I didn’t really\nget to use Swarm or Kubernetes because they exceeded my needs. I got to run my\napp in a controlled environment, with other cool services like Nginx or MongoDB\nwithout having to install them in my local machine.</p>\n<h4>Unsolved challenges</h4>\n<p>Some sites heavily use front end frameworks and do AJAX calls, which makes it\nharder for the scraper to extract data. A solution to this may be to add\nPhantomJS, but I’m afraid it would take a serious amount of RAM.</p>\n<h3>Demo and Source</h3>\n<p>Demo can be found (at the moment) on: <a href=\"http://crawlio.ml/#/\">http://crawlio.ml/</a></p>\n<p>Source:\n<a href=\"https://github.com/agustinaliagac/crawl-io/blob/master/crawlio/search/src/routes/index.js\">https://github.com/agustinaliagac/crawl-io/</a></p>\n<p><strong>PRs, critics and advise are welcome!</strong></p>\n<ul>\n<li><a href=\"https://medium.com/tag/javascript?source=post\">JavaScript</a></li>\n<li><a href=\"https://medium.com/tag/scraping?source=post\">Scraping</a></li>\n<li><a href=\"https://medium.com/tag/ecommerce?source=post\">Ecommerce</a></li>\n<li><a href=\"https://medium.com/tag/nodejs?source=post\">Nodejs</a></li>\n<li><a href=\"https://medium.com/tag/react?source=post\">React</a></li>\n</ul>\n<h3><a href=\"https://medium.com/@agustin.aliaga\">Agustin Aliaga</a></h3>\n<p>Software Developer</p>","frontmatter":{"title":"","date":null}}},"pageContext":{"slug":"/building-an-ecommerce-scraper-with-node-js-and-react-js/","previous":{"fields":{"slug":"/js-guide-for-java-devs-part-1/"},"frontmatter":{"title":"JS guide for Java developers: Part 1 — scope, closures, global context, this, and undefined"}},"next":null}}